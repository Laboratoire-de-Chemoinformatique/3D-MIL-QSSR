{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assymetric organocatalysis\n",
    "\n",
    "Asymmetric organocatalysis allows the synthesis of enantiopure compounds using organic molecules as catalysts. In 2021 B. List and D. MacMillan were awarded the Nobel Prize for the development of asymmetric organocatalysis. Recently, Denmark and coworkers published a reaction of asymmetric addition of thiols to imines catalyzed by phosphoric acids (Figure 1).\n",
    "\n",
    "<img src=\"img/figure_1.png\" width=\"450\"/>\n",
    "\n",
    "**Figure 1. Asymmetric addition of thiol to imines catalyzed by chiral phosphoric acid catalysts**\n",
    "\n",
    "# 3D Catalyst descriptors\n",
    "\n",
    "Each reaction was transformed to a Condensed Graph of Reaction (CGR) with a CGRtools package. Given CGRs were encoded by ISIDA (In Silico Design and Data Analysis) fragment descriptors.\n",
    "\n",
    "<img src=\"img/figure_2.png\" width=\"500\"/>\n",
    "\n",
    "**Figure 2. Addition of thiols to imine and related Condensed Graph of Reaction (CGR).**\n",
    "\n",
    "For each catalyst, up to 50 conformations (**nconfs**) within a 50 kcal/mol energy window (**energy**) were generated using the distance geometry algorithm implemented in RDKit.  The generated catalyst conformations were encoded with pmapper 3D descriptors. The descriptor value is defined as the number of occurrences of particular 3D atom triplet in the catalyst conformation. The atom triplet was defined by (1) the type of atoms (C, N, O, S, P, F, Cl, Br, I, 5-membered and 6-membered aromatic ring) and (2) the distance between atoms in triplet.\n",
    "\n",
    "<img src=\"img/figure_3.png\" width=\"600\"/>\n",
    "\n",
    "**Figure 3. Workflow of preparation of pmapper descriptors for a given conformation. (1) given 2D catalyst structure; (2) generation of 3D catalyst conformation and assigning atom labels; (3) generation of a 3D fully connected graph for which an ensemble of triplets is generated (for demonstration, graph of four atoms is chosen); (4) enumeration of all atom triplets; (5) counting of atom triplets in all conformations.**\n",
    "\n",
    "Vectors of 2D fragment reaction descriptors and 3D physicochemical quadruplets were then concatenated to form combined reaction/catalyst descriptor vector. \n",
    "<img src=\"img/figure_4.png\" width=\"800\"/>\n",
    "\n",
    "**Figure 4. Preparation of descriptors encoding reaction/catalyst combinations. A chemical reaction is encoded by m CGR/ISIDA descriptors. A catalyst is represented by its N conformations, each encoded by n of 3D atom triplet descriptors. Concatenation of reaction and catalyst descriptors results in the vector of (m + n) size.**\n",
    "\n",
    "# Multi-Instance learning\n",
    "\n",
    "Multi-instance (MI) machine learning approaches can be used to solve the issues of representation of each molecule by multiple conformations (instances) and automatic selection of the most relevant ones. In the multi-instance approach, an example (i.e., a molecule) is presented by a bag of instances (i.e., a set of conformations), and a label (a molecule property value) is available only for a bag (a molecule), but not for individual instances (conformations).\n",
    "\n",
    "Here, we report an application of Multi-Instance Learning approach to predictive modeling of enantioselectivity of chiral catalysts. Catalysts were represented by ensembles of conformations encoded by the *pmapper* 3D descriptors.\n",
    "\n",
    "<img src=\"img/figure_5.png\" width=\"550\"/>\n",
    "\n",
    "**Figure 5. Mutli-instance learning approach**\n",
    "\n",
    "\n",
    "# Instance-Wrapper algorithm\n",
    "\n",
    "Instance-Wrapper is the simplest instance-based MI algorithm, where each training instance (conformation) is assigned the same experimental enantioselectivity (Figure 6). As a result, we obtain a data set where each conformation is an individual training object and any conventional machine learning algorithm can be applied to build the model. Given a new catalyst, the enantioselectivity is predicted for each conformation, and then conformation predictions are averaged to derive the final predicted enantioselectivity of the catalyst (Figure 6). \n",
    "\n",
    "<img src=\"img/figure_6.png\" width=\"900\"/>\n",
    "\n",
    "**Figure 6. Instance-Wrapper algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Code installation\n",
    "\n",
    "Using **conda** and **pip** is the easiest way to install all required packages. Create a new environment (named \"exp\") with Python 3.6. Note the issues related to RDKit installation https://stackoverflow.com/questions/70202430/rdkit-importerror-dll-load-failed. <br/>\n",
    "\n",
    "Run these commands in the command line:\n",
    "\n",
    "`conda create -n exp python=3.6`<br/>\n",
    "`conda activate exp` <br/>\n",
    "\n",
    "Install RDKit package: <br/>\n",
    "\n",
    "`conda install -c conda-forge rdkit` <br/>\n",
    "\n",
    "Install PyTorch package: <br/>\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch` <br/>\n",
    "`pip install torch_optimizer` <br/>\n",
    "\n",
    "Install software to calculate 2D reactant descriptors: <br/>\n",
    "\n",
    "`pip install CGRTools CIMTools` <br/>\n",
    "\n",
    "Install software to calculate 3D catalyst descriptors: <br/>\n",
    "\n",
    "`conda install -c conda-forge openbabel` <br/>\n",
    "` pip install networkx` <br/>\n",
    "`pip install pmapper` <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data set preparation\n",
    "\n",
    "The reactant transformations and catalyst structures are recorded in an RDF file, which has a fixed structure: \n",
    "\n",
    "**Reactants mol blocks** - encoded reactants transformation <br/>\n",
    "\n",
    "Metadata fields `$DTYPE-$DATUM`:<br/>\n",
    "**ID** - catalyst ID <br/>\n",
    "**SELECTIVITY** - experimental catalyst selectivity <br/>\n",
    "**CATALYST_SMILES** - сatalyst structure in SMILES <br/>\n",
    "\n",
    "You should prepare your data set in the described form, also see example RDF file in `data/input_data.rdf`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Descriptor calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from miqssr.utils import calc_3d_descr\n",
    "\n",
    "input_fname = os.path.join('data', 'input_data.rdf') # data\n",
    "nconfs = [5] # max number of conformers to generate\n",
    "energy = 50 # energy window\n",
    "ncpu = 20 # number of cpus\n",
    "path = './descriptors' # where to store the calculated descriptors\n",
    "\n",
    "out_fname = calc_3d_descr(input_fname=input_fname, nconfs=nconfs, energy=energy, ncpu=ncpu, path=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descriptor folder contains several files:\n",
    "\n",
    "`2DDescrISIDA_cgr-data_0.csv` - fragment descriptors of reactant transformations<br/>\n",
    "\n",
    "`conf-catalyst_data_5.pkl` - pickle file with RDKit the generated conformations<br/>\n",
    "`conf-50_catalyst_data_log.pkl` - pickle file with the conformation energies<br/>\n",
    "\n",
    "`PhFprPmapper_conf-catalyst_data_5.txt` - SVM file with pmapper 3D descriptors for generated conformations<br/>\n",
    "`PhFprPmapper_conf-catalyst_data_5.colnames` - names of pmapper 3D descriptors for generated conformations<br/>\n",
    "`PhFprPmapper_conf-catalyst_data_5.rownames` - id of generated conformations<br/>\n",
    "\n",
    "`PhFprPmapper_concat_data_5.txt` - SVM file with concatenated reactant 2D fragment descriptors and pmapper 3D descriptors<br/>\n",
    "`PhFprPmapper_concat_data_5.rownames` - reaction id and experimental enantioselectivity<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparation of training and test set\n",
    "\n",
    "Reaction descriptors are stored in a dense SVM format. One should to implement a function to create a n × m × k list of bags (n - number of reactions, m - bag size (number of conformations generated), k - number of descriptors).\n",
    "\n",
    "Training set containes 384 reactions (24 catalysts × 16 reactant combinations = 384 reactions), and the external test set containes 691 reactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_svm_data(fname):\n",
    "    \n",
    "    def str_to_vec(dsc_str, dsc_num):\n",
    "\n",
    "        tmp = {}\n",
    "        for i in dsc_str.split(' '):\n",
    "            tmp[int(i.split(':')[0])] = int(i.split(':')[1])\n",
    "        #\n",
    "        tmp_sorted = {}\n",
    "        for i in range(dsc_num):\n",
    "            tmp_sorted[i] = tmp.get(i, 0)\n",
    "        vec = list(tmp_sorted.values())\n",
    "\n",
    "        return vec\n",
    "    \n",
    "    #\n",
    "    with open(fname) as f:\n",
    "        dsc_tmp = [i.strip() for i in f.readlines()]\n",
    "\n",
    "    with open(fname.replace('txt', 'rownames')) as f:\n",
    "        react_names = [i.strip() for i in f.readlines()]\n",
    "    #\n",
    "    labels_tmp = [float(i.split(':')[1]) for i in react_names]\n",
    "    idx_tmp = [i.split(':')[0] for i in react_names]\n",
    "    dsc_num = max([max([int(j.split(':')[0]) for j in i.strip().split(' ')]) for i in dsc_tmp])\n",
    "    #\n",
    "    bags, labels, idx = [], [], []\n",
    "    for react_idx in list(np.unique(idx_tmp)):\n",
    "        bag, labels_, idx_ = [], [], []\n",
    "        for dsc_str, label, i in zip(dsc_tmp, labels_tmp, idx_tmp):\n",
    "            if i == react_idx:\n",
    "                bag.append(str_to_vec(dsc_str, dsc_num))\n",
    "                labels_.append(label)\n",
    "                idx_.append(i)\n",
    "                \n",
    "        bags.append(np.array(bag).astype('uint8'))\n",
    "        labels.append(labels_[0])\n",
    "        idx.append(idx_[0])\n",
    "\n",
    "    return np.array(bags), np.array(labels), np.array(idx)\n",
    "\n",
    "\n",
    "def train_test_split_function(bags, labels, idx, test_set='default'):\n",
    "\n",
    "    with open(os.path.join('data', 'test_sets.pickle'), 'rb') as f:\n",
    "        test_reactions_idx = pickle.load(f)[test_set]\n",
    "\n",
    "    x_train, x_test = [], []\n",
    "    y_train, y_test = [], []\n",
    "    idx_train, idx_test = [], []\n",
    "    for bag, label, i in zip(bags, labels, idx):\n",
    "        if i in test_reactions_idx:\n",
    "            x_test.append(bag)\n",
    "            y_test.append(label)\n",
    "            idx_test.append(i)\n",
    "        else:\n",
    "            x_train.append(bag)\n",
    "            y_train.append(label)\n",
    "            idx_train.append(i)\n",
    "            \n",
    "    x_train, x_test, y_train, y_test = np.array(x_train), np.array(x_test), np.array(y_train), np.array(y_test)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test, idx_train, idx_test\n",
    "\n",
    "\n",
    "def train_test_split_default(bags, labels, idx):\n",
    "    return train_test_split_function(bags, labels, idx, test_set='default')\n",
    "\n",
    "\n",
    "# split data into a training and test set\n",
    "dsc_fname = os.path.join('descriptors', 'PhFprPmapper_concat_data_5.txt') # descriptors file\n",
    "bags, labels, idx = load_svm_data(dsc_fname)\n",
    "print(f'There are {len(bags)} reactions encoded with {bags[0].shape[1]} descriptors')\n",
    "x_train, x_test, y_train, y_test, idx_train, idx_test = train_test_split_default(bags, labels, idx)\n",
    "print(f'There are {len(x_train)} training reactions and {len(x_test)} test reactions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better training of the neural network, the descriptors should be scaled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scale_data(x_train, x_test):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(np.vstack(x_train))\n",
    "    x_train_scaled = x_train.copy()\n",
    "    x_test_scaled = x_test.copy()\n",
    "    for i, bag in enumerate(x_train):\n",
    "        x_train_scaled[i] = scaler.transform(bag)\n",
    "    for i, bag in enumerate(x_test):\n",
    "        x_test_scaled[i] = scaler.transform(bag)\n",
    "    return np.array(x_train_scaled), np.array(x_test_scaled)\n",
    "\n",
    "\n",
    "x_train_scaled, x_test_scaled = scale_data(x_train, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One should implement a protocol for optimizing the hyperparameters of the neural network. Here we assign the optimal hyperparameters found with the grid search technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from miqssr.estimators.wrappers import InstanceWrapperMLPRegressor\n",
    "from miqssr.estimators.utils import set_seed\n",
    "set_seed(43)\n",
    "\n",
    "ndim = (x_train_scaled[0].shape[1], 256, 128, 64) # number of hidden layers and neurons in the main network\n",
    "pool = 'mean'                                       # type of pulling of instance descriptors\n",
    "n_epoch = 1000                                       # maximum number of learning epochs\n",
    "lr = 0.001                                          # learning rate\n",
    "weight_decay = 0.001                                # l2 regularization\n",
    "batch_size = 99999999                               # batch size\n",
    "init_cuda = True                                    # True if GPU is available\n",
    "\n",
    "\n",
    "net = InstanceWrapperMLPRegressor(ndim=ndim, pool=pool, init_cuda=init_cuda)\n",
    "net.fit(x_train_scaled, y_train, \n",
    "        n_epoch=n_epoch, \n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model is then used to predict enantioselectivity on 3 test sets (see details in original paper):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "results_df['IDX'] = idx_test\n",
    "results_df['OBS'] = y_test\n",
    "results_df['PRED'] = net.predict(x_test_scaled)\n",
    "results_df.set_index(['IDX'], inplace=True)\n",
    "\n",
    "# results\n",
    "with open(os.path.join('data', 'test_sets.pickle'), 'rb') as f:\n",
    "    test_sets = pickle.load(f)\n",
    "\n",
    "print('Test scenario 1: 9 test reactions × 24 training catalysts = 216 test data points')\n",
    "tmp = results_df.loc[test_sets['test_1']]\n",
    "r2 = r2_score(tmp['OBS'], tmp['PRED'])\n",
    "mae = mean_absolute_error(tmp['OBS'], tmp['PRED'])\n",
    "print(f'    R2 = {r2:.2f}')\n",
    "print(f'    MAE = {mae:.2f} kcal/mol')\n",
    "\n",
    "print('Test scenario 2: 9 training reactions × 19 test catalysts = 304 test data points')\n",
    "tmp = results_df.loc[test_sets['test_2']]\n",
    "r2 = r2_score(tmp['OBS'], tmp['PRED'])\n",
    "mae = mean_absolute_error(tmp['OBS'], tmp['PRED'])\n",
    "print(f'    R2 = {r2:.2f}')\n",
    "print(f'    MAE = {mae:.2f} kcal/mol')\n",
    "\n",
    "print('Test scenario 3: 9 test reactions × 19 test catalysts = 171 data points')\n",
    "tmp = results_df.loc[test_sets['test_3']]\n",
    "r2 = r2_score(tmp['OBS'], tmp['PRED'])\n",
    "mae = mean_absolute_error(tmp['OBS'], tmp['PRED'])\n",
    "print(f'    R2 = {r2:.2f}')\n",
    "print(f'    MAE = {mae:.2f} kcal/mol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp",
   "language": "python",
   "name": "exp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}